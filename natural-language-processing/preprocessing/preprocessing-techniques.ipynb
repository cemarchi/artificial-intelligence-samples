{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('python38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4ff4e1a0e02381a97fe12382f82451566ed88226ec2dd2496bdb143e4d857f47"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Técnicas de pré-processamento de texto para NLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<p>Este é um guia de exemplos das principais de técnicas de pré-processamento de texto para NLP. Aplicando as técnicas de pré-processamento de texto, podemos remover o ruído dos dados brutos e tornar os dados brutos mais valiosos para a construção de modelos.</p>\n",
    "<p>Os dados brutos para NLP nada mais são que textos coletados de alguma fonte de interesse, podendo ser avaliações de sites, documentos, mídia social, tweets do Twitter, artigos de notícias, produtos de um marketplace, e-mails, dentre outros.</p>\n",
    "<p>O pré-processamento de dados é a etapa principal e extremamente importante para qualquer projeto que envolva ciência de dados, incluindo não somente NLP, mas sobre tudo aprendizado de máquina, aprendizado profundo e visão computacional. Dessa forma, essas técnicas podem variar de acordo com o campo da inteligência artifical, por exemplo técnicas de pré-processamento de dados de aprendizado de máquina variam de aprendizado profundo, linguagem natural e visão computacional.</p>\n",
    "<p>Vejamos as diferentes técnicas ou métodos de pré-processamento de texto que podemos aplicar na NLP:</p>\n",
    "\n",
    "- Conversão de texto para minúscula\n",
    "- Remoção de <i>tags</i> HTML\n",
    "- Removeção de URLs\n",
    "- Conversão de números para palavras\n",
    "- Remoção de caracteres acentuados\n",
    "- Correção ortográfica\n",
    "- Remoção de caracteres de pontuação\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conversão de texto para minúscula"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<p>É uma abordagem simples e eficaz, pois a conversão torna-se necessária para reduzir a dimensionalidade evitando palavras iguais, como NLP, Nlp ou nlp sejam tratadas como palavras distintas.</p>\n",
    "<p>Este método é útil para problemas que utilizam frequência de palavras, como classificação de documentos, por exemplo <i><a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">Bag of Words</a></i>, <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">TFIDF</a>, <a  href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2Vec</a>, <a href=\"https://en.wikipedia.org/wiki/GloVe_(machine_learning)\">GloVe</a> e <a href=\"https://en.wikipedia.org/wiki/FastText\">fastText</a>. Por outro lado, não sendo indicado para o reconhecimento de <i>tag</i> de partes da fala (<a href=\"https://en.wikipedia.org/wiki/Part-of-speech_tagging\">PoS tagging</a>) ou <a href=\"https://medium.com/@5hirish/dependency-parsing-in-nlp-d7ade014186\">análise de dependência</a>, onde a capitalização adequada de palavras é essencial para reconhecer substantivos, verbos, etc.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower_case(text):\n",
    "\treturn text.lower()"
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "este é um exemplo de texto nlp.\n"
     ]
    }
   ],
   "source": [
    "text = 'Este é um exemplo de texto NLP.'\n",
    "print(convert_to_lower_case(text))"
   ]
  },
  {
   "source": [
    "## Remoção de <i>tags</i> HTML"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<p>Quando o texto contém <i>tags</i> HTML é necessário removê-las, pois não há informação valiosa. Os meios mais comuns de removê-las é usando regex ou também pelo módulo <b>BeautifulSoup</b> da biblioteca <b><a href=\"https://pypi.org/project/bs4/\">bs4</a></b>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags(text):\n",
    "\tparser = BeautifulSoup(text, 'html.parser')\n",
    "\treturn parser.get_text(separator = ' ')"
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n \n Produtos encontrados: IPhone. \n IPhone 10. \n \n \n\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"<body>\n",
    "<div>\n",
    "<h1>Produtos encontrados: IPhone.</h1>\n",
    "<h3>IPhone 10.</h3>\n",
    "</div>\n",
    "</body>\n",
    "\"\"\"\n",
    "print(remove_html_tags(text))"
   ]
  },
  {
   "source": [
    "## Remoção de URLs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "\turl_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\treturn re.sub(pattern=url_pattern, repl='', string=text)"
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Por favor, pesquise no Google por NLP para contrar mais informações  Tudo bem?\n\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Por favor, pesquise no Google por NLP para contrar mais informações https://www.google.com/search?q=nlp&oq=nlp&aqs=edge.0.69i59l3j69i60l3j0.1045j0j4&sourceid=chrome&ie=UTF-8. Tudo bem?\n",
    "\"\"\"\n",
    "\n",
    "print(remove_urls(text))"
   ]
  },
  {
   "source": [
    "## Conversão de números para palavras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<p>Esse tipo de conversão pode ser interessante aplicar quando os números são importantes para o contexto. Utilizamos a biblioteca <b><a href=\"https://pypi.org/project/num2words/\">num2words</a></b> para fazer a conversão de número para palavra.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "def numbers_to_words(text, lang='pt_BR'):\n",
    "    words = text.split()\n",
    "    \n",
    "    for word_index in filter(lambda word_index: words[word_index].isdigit(), range(len(words))):        \n",
    "        words[word_index] = num2words(words[word_index], lang)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Este produto dou nota one thousandth !!! Sensacional! Vou comprar mais third !:)\n"
     ]
    }
   ],
   "source": [
    "texto = 'Este produto dou nota 1000 !!! Sensacional! Vou comprar mais 3 !:)'\n",
    "print(numbers_to_words(texto))"
   ]
  },
  {
   "source": [
    "## Remoção de caracteres acentuados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<p>Quando os caracteres especiais não são removidos do texto, o modelo identifica palavras acenturadas distintas, como pröva e prova. Porém, pode ser complicado utilizar essa técnica com palavras no idioma português, pois algumas palavras acentuadas possuem significados diferentes e dependendo do problema em questão, elas devem manter essa distinção, como pais e país. Nesse caso, outra técnica por ser melhor empregada, por exemplo  correção ortográfica.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "def remove_accented_characters(text):\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Este foi o 10o e ultimo produto \"congelado\". Ele estava 30degC.\n"
     ]
    }
   ],
   "source": [
    "text = 'Este foi o 10º e último produto congelado. Ele estava 30°C.'\n",
    "print(remove_accented_characters(text))"
   ]
  },
  {
   "source": [
    "## Correção ortográfica"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<p>Importante técnica para reduzir o ruíno no texto. Há alguns pacotes disponíveis em Python, porém nem todos funcionam bem utilizando o idioma Português, talvez isso pode ser resolvido incluindo mais palavras ao dicionário. Uma boa biblioteca em Python para correção ortográfica é a <b><a href=\"https://github.com/fsondej/autocorrect\">autocorrect</a></b> que permite incluir outros idiomas ou até mesmo melhorar as existentes de forma fácil.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "def correct_spell(text, lang='pt'):\n",
    "    speller = Speller(lang)\n",
    "    return speller(text) "
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vamos corrigir este resto com palavras e tambem sem acentuação.\n"
     ]
    }
   ],
   "source": [
    "text = 'Vams corigir este testo com palvcras e tambem sem acentuaçao.'\n",
    "print(correct_spell(text))"
   ]
  },
  {
   "source": [
    "## Remoção de caracteres de pontuação"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def remove_punctuation(text):\n",
    "\treturn text.translate(str.maketrans('', '', punctuation))"
   ]
  },
  {
   "source": [
    "### Exemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Este produto é otimo fácil intalação Tenho uma dúvida qual é a voltagem do produto Vontagem 110v 220v \n"
     ]
    }
   ],
   "source": [
    "text = 'Este produto é otimo (fácil intalação)!!!!! Tenho uma dúvida, qual é a voltagem do produto? Vontagem: 110v, 220v. '\n",
    "print(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}